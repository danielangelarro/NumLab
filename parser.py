"""
This module contains the basic structures for parsing.
"""

from __future__ import annotations

from typing import List

from exceptions import ParsingError
from generic_ast import AST
from grammar import Grammar, Terminal
from terminal_set import TerminalSet
from tokenizer import Token, Tokenizer


class Parser:
    """Structure used for parsing a text given a grammar and a tokenizer.

    Parameters
    ----------
    grammar : Grammar
        Grammar that will be use for parsing.
    tokenizer : Tokenizer
        Tokenizer that will be use for tokenize a given txt.
    """

    def __init__(self, grammar: Grammar, tokenizer: Tokenizer = None):
        self.grammar = grammar
        self.tokenizer = tokenizer
        self._first = None
        self._prod_first = None
        self._follow = None
        self._ll_one_table = None
        self.token_to_term = {}

    def _calcule_first_and_follow(self):
        """Recalculates the `first` and `follow` sets of the grammar."""
        self.calculate_follow()

    def _build_ll_one_table(self):
        """Builds the LL(1) table."""
        table = self._ll_one_table = {}

        all_terminals = self.grammar.all_terminals()
        all_terminals.add(Terminal("$"))
        for expr, prod in self.grammar.all_productions():
            if prod.is_eps:
                continue
            for terminal in all_terminals:
                if terminal == "EPS":
                    continue
                if terminal in self._prod_first[prod]:
                    if (expr, terminal) in table and table[expr, terminal] is not None:
                        raise ValueError(
                            f"Ambiguity in the LL(1) table: \n"
                            f"{expr} {prod}\n"
                            f"{expr} {table[expr, terminal]}"
                        )
                    table[expr, terminal] = prod
                elif "EPS" in self._first[expr] and terminal.name in self._follow[expr]:
                    table[expr, terminal] = "EPS"
                elif (expr, terminal) not in table:
                    table[expr, terminal] = None

    def _build_token_to_term_dict(self):
        """
        Builds a dictionary that maps a token from the tokenizer to a
        grammar terminal.
        """
        all_terminals = self.grammar.all_terminals()
        for token_type, patt in self.tokenizer.token_patterns.items():
            for term in all_terminals:
                if term.is_literal and patt.match(term.match):
                    self.token_to_term[token_type] = term
                    break
                if not term.is_literal and token_type == term.name:
                    self.token_to_term[token_type] = term
                    break

    def parse_file(self, file_path: str) -> AST:
        """Opens a file and parses it contents.

        Parameters
        ----------
        file_path : str
            File path.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        with open(file_path, "r", encoding="utf-8") as file:
            text = file.read()
        return self.parse(text)

    def parse(self, text: str) -> AST:
        """Parses a text.

        Parameters
        ----------
        text : str
            Text to be parsed.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        tokens = self.tokenizer.tokenize(text)
        return self.parse_tokens(tokens)

    def parse_tokens(self, tokens: List[Token]) -> AST:
        """Parses a list of tokens.

        Parameters
        ----------
        tokens : List[Token]
            List of tokens to be parsed.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        self.calculate_follow()
        self._build_ll_one_table()
        last_line, last_col = (tokens[-1].line, tokens[-1].col) if tokens else (0, 0)
        tokens.append(Token("$", "$", last_line, last_col))
        derivation_root = AST(self.grammar.start_expr)
        derivation_stack = [derivation_root]
        self._build_token_to_term_dict()

        last_token = None
        for token in tokens:
            last_token = token
            if token.token_type == "$":
                while (
                    len(derivation_stack) > 0
                    and "EPS" in self._first[derivation_stack[-1].item]
                ):
                    derivation_stack.pop()
                if len(derivation_stack) != 0:
                    raise ParsingError(
                        "Unexpected end of file",
                        last_line,
                        last_col,
                    )
                return derivation_root

            while True:
                if len(derivation_stack) == 0:
                    raise ParsingError("Unespected ending", token.line, token.col)
                derivation = derivation_stack.pop()
                if derivation.item.is_terminal:
                    derivation.value = token.lexem
                    break

                term_name = self.token_to_term[token.token_type]
                prod_to_apply = self._ll_one_table[derivation.item, term_name]
                if prod_to_apply is None:
                    raise ParsingError(
                        f"Unexpected '{token}'",
                        token.line,
                        token.col,
                    )
                if prod_to_apply == "EPS":
                    continue

                for item in prod_to_apply.items[::-1]:
                    new_derivation = AST(item, derivation)
                    derivation.children.insert(0, new_derivation)
                    derivation_stack.append(new_derivation)
        if len(derivation_stack) != 0:
            raise ParsingError(
                f"Unexpected {last_token.lexem}", last_token.line, last_token.col
            )

        return derivation_root

    def calculate_first(self):
        """Calculates the `first` set of the grammar."""

        self._first = {expr: TerminalSet() for expr in self.grammar.exprs}
        self._prod_first = {
            prod: TerminalSet() for _, prod in self.grammar.all_productions()
        }

        change = True

        while change:
            change = False
            for expr, prod in self.grammar.all_productions():
                for item in prod.items:
                    if item.is_terminal:
                        change |= self._first[expr].add(item)
                        self._prod_first[prod].add(item)
                        break
                    if item != expr:
                        change |= self._first[expr].update(self._first[item])
                        self._prod_first[prod].update(self._first[item])
                        if "EPS" not in self._first[item].terminals:
                            break

    def calculate_follow(self):
        """Calculates `follow` set of the grammar."""

        # First is needed to calculate follow
        self.calculate_first()

        follow = {expr: TerminalSet() for expr in self.grammar.exprs}
        follow[self.grammar.start_expr].add(Terminal("$"))

        change = True
        while change:
            change = False
            for expr, prod in self.grammar.all_productions():
                for i, item in enumerate(prod.items):
                    next_item = prod.items[i + 1] if i + 1 < len(prod.items) else None
                    if item.is_terminal:
                        continue
                    if next_item is None:
                        change |= follow[item].update(follow[expr] - "EPS")
                    elif next_item.is_terminal:
                        change |= follow[item].add(next_item)
                    else:
                        change |= follow[item].update(self._first[next_item] - "EPS")
                        if "EPS" not in self._first[next_item].terminals:
                            break
        self._follow = follow
