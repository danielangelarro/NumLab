"""
This module contains the basic structures for parsing.
"""

from __future__ import annotations

import logging
from typing import Dict, List, Tuple

from automata import Automata, State
from exceptions import ParsingError
from generic_ast import AST
from grammar import Grammar, Item, NonTerminal, Production, Terminal
from terminal_set import TerminalSet
from tokenizer import Token, Tokenizer

_REDUCE_ACTION = 0
_SHIFT_ACTION = 1


class SLRItem:
    """
    This class represents a single item in the LR(0) state machine.
    """

    def __init__(self, prod: Production, dot_pos: int):
        """
        Initializes a new SLRItem.

        Parameters
        ----------
        production : Production
            Production that will be used.
        dot_pos : int
            Position of the dot in the production.
        """
        self.prod = prod
        self.dot_pos = dot_pos

    def __repr__(self):
        """
        Returns a string representation of the item.

        Returns
        -------
        str
            String representation of the item.
        """
        head = f"{self.prod.head} -> "
        body_before_dot = " ".join(str(i) for i in self.prod.items[: self.dot_pos])
        body_after_dot = " ".join(str(i) for i in self.prod.items[self.dot_pos :])
        return f"{head}{body_before_dot} . {body_after_dot}"


class Parser:
    """Structure used for parsing a text given a grammar and a tokenizer.

    Parameters
    ----------
    grammar : Grammar
        Grammar that will be use for parsing.
    tokenizer : Tokenizer
        Tokenizer that will be use for tokenize a given txt.
    """

    def __init__(self, grammar: Grammar, tokenizer: Tokenizer = None):
        self.grammar = grammar
        self.tokenizer = tokenizer
        self._first = None
        self._prod_first = None
        self._follow = None
        self.token_to_term = {}
        self._slr_atmt_nfa = None
        self._stt2item = None

    def _calculate_first_and_follow(self):
        """Recalculates the `first` and `follow` sets of the grammar."""
        self.calculate_follow()

    def _build_slr_atmt(self):
        if self._slr_atmt_nfa is not None and self._stt2item is not None:
            return self._slr_atmt_nfa, self._stt2item

        logging.debug("Building SLR automata")

        # Prepare grammar
        logging.debug("Preparing grammar (adding S')")
        if "S`" not in self.grammar.exprs_dict:
            non_ter_prod = Production([self.grammar.start])
            non_ter = NonTerminal("S`", [non_ter_prod])
            self.grammar.add_expr(non_ter)
            self.grammar.start = non_ter
            self.grammar.start.prod_0.set_builder(lambda s: s.ast)

        # Extract all slr items
        logging.debug("Extracting all SLR items")
        slr_items = []
        slr_item_dict = {}
        for _, prod in self.grammar.all_productions():
            for dot_pos in range(len(prod.items) + 1):
                slr_item = SLRItem(prod, dot_pos)
                slr_items.append(slr_item)
                slr_item_dict[prod, dot_pos] = slr_item
        logging.debug(f"Found {len(slr_items)} SLR items")
        logging.debug("Exacted items in the SLR automata:")
        for slr_item in slr_items:
            logging.debug(f"  {slr_item}")

        # Build the SLR state machine
        logging.debug("Adding states to automata")
        stt2item, item2stt = {}, {}
        atmt = Automata()
        for i, slr_item in enumerate(slr_items):
            stt = atmt.add_state(
                f"q{i}",
                start=slr_item.prod.head.name == "S`" and slr_item.dot_pos == 0,
                end=True,
                name=str(slr_item),
            )
            stt2item[stt] = slr_item
            item2stt[slr_item] = stt

        logging.debug("Adding transitions to automata")
        for stt in atmt.states.values():
            slr_item = stt2item[stt]
            if slr_item.dot_pos == len(slr_item.prod.items):
                continue
            gm_item = slr_item.prod.items[slr_item.dot_pos]
            next_item = slr_item_dict[slr_item.prod, slr_item.dot_pos + 1]
            atmt.add_transition(stt, item2stt[next_item], gm_item.name)
            all_slr_items_head_gm = [
                slr_item_dict[p, 0]
                for _, p in self.grammar.all_productions()
                if p.head == gm_item
            ]
            for slr_item_gm in all_slr_items_head_gm:
                atmt.add_transition(stt, item2stt[slr_item_gm])

        self._slr_atmt_nfa = atmt
        self._stt2item = stt2item
        return atmt, stt2item

    def parse_file(self, file_path: str) -> AST:
        """Opens a file and parses it contents.

        Parameters
        ----------
        file_path : str
            File path.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        with open(file_path, "r", encoding="utf-8") as file:
            text = file.read()
        return self.parse(text)

    def parse(self, text: str) -> AST:
        """Parses a text.

        Parameters
        ----------
        text : str
            Text to be parsed.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        tokens = self.tokenizer.tokenize(text)
        return self.parse_tokens(tokens)

    def parse_tokens(self, tokens: List[Token]) -> AST:
        """Parses a list of tokens.

        Parameters
        ----------
        tokens : List[Token]
            List of tokens to be parsed.
        method : str
            Method used for parsing.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        tokens += [Token("$", "'$'")]
        return self._slr_parse_tokens(tokens)

    def _slr_parse_tokens(self, tokens: List[Token]) -> AST:
        """Parses a list of tokens.

        Parameters
        ----------
        tokens : List[Token]
            List of tokens to be parsed.

        Returns
        -------
        AST
            AST generated by the parser.
        """
        logging.debug("Parsing tokens using SLR method")

        # Build slr nfa automata
        slr_atmt_nfa, stt2item = self._build_slr_atmt()

        # Calculate first and follows
        self._calculate_first_and_follow()

        # Build slr dfa automata from slr nfa automata
        logging.debug("Building SLR DFA automata")
        slr_atmt, dfa2nfa = slr_atmt_nfa.to_dfa(dfa2nfa=True)
        current_state = slr_atmt.start_state

        # State to SRL items dictionary
        logging.debug("Building state to SRL items dictionary")
        state_items: Dict[State, List[SLRItem]] = {}
        for state, old_states in dfa2nfa.items():
            state_items[state] = []
            for old_state in old_states:
                state_items[state].append(stt2item[old_state])

        # Initialize stack
        logging.debug("Initializing stack")
        stack: List[Tuple[Item, State]] = []

        i = 0
        logging.debug("Parsing tokens")
        while i < len(tokens):
            token = tokens[i]
            current_state = slr_atmt.start_state if not stack else stack[-1][1]
            action: int = None
            reduce_prod: Production = None

            # Decide between shift and reduce
            posible_items = state_items[current_state]
            for item in posible_items:
                if item.dot_pos == len(item.prod.items) and self._follow[
                    item.prod.head.name
                ].has_token(token):
                    if action is None or action == _REDUCE_ACTION:
                        action = _REDUCE_ACTION
                        reduce_prod = item.prod
                    else:
                        raise ParsingError("Ambiguity in the SLR", token)
                elif item.dot_pos < len(item.prod.items) and item.prod.items[
                    item.dot_pos
                ].check_token(token):
                    if action is None:
                        action = _SHIFT_ACTION
                    else:
                        raise ParsingError("Ambiguity in the SLR", token)

            if action == _REDUCE_ACTION:
                # Pop from stack the necessary items
                items_needed = len(reduce_prod.items)
                stack, items = stack[:-items_needed], stack[-items_needed:]
                items = [item[0] for item in items]

                # Apply reduction
                new_head = reduce_prod.head.copy()
                new_head.ast = reduce_prod.build_ast(items)

                # Check next state
                current_state = slr_atmt.start_state if not stack else stack[-1][1]
                next_state = current_state.next_state(new_head.name)

                # Push to stack the new item
                stack.append((new_head, next_state))
                if new_head.name == "S`":
                    break
            elif action == _SHIFT_ACTION:
                i += 1
                next_state = current_state.next_state(token.token_type)
                term = Terminal(token.token_type, value=token.lexem)
                if next_state is None:
                    raise ParsingError("Invalid token", token)
                stack.append((term, next_state))
            else:
                raise ParsingError("Invalid token", token)
        if len(stack) != 1:
            raise ParsingError("Invalid tokens", tokens[-1])
        return stack[0][0].ast

    def calculate_first(self):
        """Calculates the `first` set of the grammar."""

        self._first = {expr.name: TerminalSet() for expr in self.grammar.exprs}
        self._prod_first = {
            prod: TerminalSet() for _, prod in self.grammar.all_productions()
        }

        change = True

        while change:
            change = False
            for expr, prod in self.grammar.all_productions():
                for item in prod.items:
                    if item.is_terminal:
                        change |= self._first[expr.name].add(item)
                        self._prod_first[prod].add(item)
                        break
                    if item != expr:
                        change |= self._first[expr.name].update(self._first[item.name])
                        self._prod_first[prod].update(self._first[item.name])
                        if "EPS" not in self._first[item.name].terminals:
                            break

    def calculate_follow(self):
        """Calculates `follow` set of the grammar."""

        # First is needed to calculate follow
        self.calculate_first()

        follow = {expr.name: TerminalSet() for expr in self.grammar.exprs}
        follow[self.grammar.start_expr.name].add(Terminal("$"))

        change = True
        while change:
            change = False
            for expr, prod in self.grammar.all_productions():
                for i, item in enumerate(prod.items):
                    next_item = prod.items[i + 1] if i + 1 < len(prod.items) else None
                    if item.is_terminal:
                        continue
                    if next_item is None:
                        change |= follow[item.name].update(follow[expr] - "EPS")
                    elif next_item.is_terminal:
                        change |= follow[item.name].add(next_item)
                    else:
                        change |= follow[item.name].update(
                            self._first[next_item] - "EPS"
                        )
                        if "EPS" not in self._first[next_item].terminals:
                            break
        self._follow = follow
